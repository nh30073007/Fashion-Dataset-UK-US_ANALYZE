{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd1e1754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 225s 9ms/step - loss: 0.0839\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 226s 9ms/step - loss: 0.0834\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 209s 8ms/step - loss: 0.0834\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 222s 9ms/step - loss: 0.0834\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 231s 9ms/step - loss: 0.0834\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 258s 10ms/step - loss: 0.0834\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 249s 10ms/step - loss: 0.0834\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 243s 10ms/step - loss: 0.0834\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 250s 10ms/step - loss: 0.0834\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 220s 9ms/step - loss: 0.0834\n",
      "Enter the brand: Ted Baker\n",
      "Enter the category: Swimwear\n",
      "1/1 [==============================] - 1s 681ms/step\n",
      "Predicted Price: 54.57593\n",
      "Do you want to continue? (yes/no): YES\n",
      "Enter the brand: Jigsaw\n",
      "Enter the category: Tops\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Predicted Price: 28.863153\n",
      "Do you want to continue? (yes/no): YES\n",
      "Enter the brand: Ralph Lauren\n",
      "Enter the category: Tops\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted Price: 54.57587\n",
      "Do you want to continue? (yes/no): YES\n",
      "Enter the brand: Alexander McQueen\n",
      "Enter the category: Outerwear\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted Price: 54.575817\n",
      "Do you want to continue? (yes/no): NO\n"
     ]
    }
   ],
   "source": [
    "# BUILD ITERECT TO USER USING RNNs MODEL , WHEN USER SUBMIT BRAND AND CETEGORY NAME HE GET REPLY PREDICTED PRICE ABOUT \n",
    "#THOSE PRODUCT\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# DATASET\n",
    "df = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\FASION DATASET UK AND US\\mock_fashion_data_uk_us.csv\")\n",
    "\n",
    "# REMOVE MISING VALUES\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# REMOVE DUPLICATES\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# REMOVE URLs\n",
    "df['Customer Reviews'] = df['Customer Reviews'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "df['Product Name'] = df['Product Name'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "df['Social Media Comments'] = df['Social Media Comments'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "df['feedback'] = df['feedback'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "df['Brand'] = df['Brand'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))\n",
    "df['Category'] = df['Category'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))\n",
    "\n",
    "# REMOVE SPECIAL CHERECHTER\n",
    "df['Customer Reviews'] = df['Customer Reviews'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['Product Name'] = df['Product Name'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['Social Media Comments'] = df['Social Media Comments'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['feedback'] = df['feedback'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['Brand'] = df['Brand'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
    "df['Category'] = df['Category'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))\n",
    "\n",
    "# CONVERT TEXT INTO LOWER CASE\n",
    "df['Customer Reviews'] = df['Customer Reviews'].apply(lambda x: x.lower())\n",
    "df['Product Name'] = df['Product Name'].apply(lambda x: x.lower())\n",
    "df['Social Media Comments'] = df['Social Media Comments'].apply(lambda x: x.lower())\n",
    "df['feedback'] = df['feedback'].apply(lambda x: x.lower())\n",
    "df['Brand'] = df['Brand'].str.lower()\n",
    "df['Category'] = df['Category'].str.lower()\n",
    "\n",
    "# REMOVE STOP WORDS\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Customer Reviews'] = df['Customer Reviews'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "df['Product Name'] = df['Product Name'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "df['Social Media Comments'] = df['Social Media Comments'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "df['feedback'] = df['feedback'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "df['Brand'] = df['Brand'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "df['Category'] = df['Category'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stop_words]))\n",
    "\n",
    "# STEMMING\n",
    "stemmer = PorterStemmer()\n",
    "df['Customer Reviews'] = df['Customer Reviews'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df['Product Name'] = df['Product Name'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df['Social Media Comments'] = df['Social Media Comments'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df['feedback'] = df['feedback'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df['Brand'] = df['Brand'].apply(lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()]))\n",
    "df['Category'] = df['Category'].apply(lambda x: ' '.join([stemmer.stem(word) for word in str(x).split()]))\n",
    "\n",
    "# FILTER RELEVENT COLUMN\n",
    "df = df[['Brand', 'Category', 'Price']]\n",
    "\n",
    "# CREATE INPUT SEQUENCE\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Brand'] + ' ' + df['Category'])\n",
    "sequences = tokenizer.texts_to_sequences(df['Brand'] + ' ' + df['Category'])\n",
    "X = pad_sequences(sequences)\n",
    "\n",
    "# NORMALIZE PRICE COLUMN\n",
    "scaler = MinMaxScaler()\n",
    "y = scaler.fit_transform(df['Price'].values.reshape(-1, 1))\n",
    "\n",
    "# SPLIT DATA \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# RNNs MODEL\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X.shape[1]))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# TRAIN MODEL\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# FUNCTION TO PREDICT PRICE\n",
    "def predict_price(brand, category):\n",
    "    input_text = brand.lower() + ' ' + category.lower()\n",
    "    input_sequence = tokenizer.texts_to_sequences([input_text])\n",
    "    input_sequence = pad_sequences(input_sequence, maxlen=X.shape[1])\n",
    "    predicted_price = scaler.inverse_transform(model.predict(input_sequence))\n",
    "    return predicted_price[0][0]\n",
    "\n",
    "# INTERECT WITH USER\n",
    "while True:\n",
    "    brand = input(\"Enter the brand: \")\n",
    "    category = input(\"Enter the category: \")\n",
    "    predicted_price = predict_price(brand, category)\n",
    "    print(\"Predicted Price:\", predicted_price)\n",
    "    continue_interaction = input(\"Do you want to continue? (yes/no): \")\n",
    "    if continue_interaction.lower() != 'yes':\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1e6ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
